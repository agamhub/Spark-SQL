{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "C5YU-yfN95MH"
      ],
      "authorship_tag": "ABX9TyNjlzwjU7VfDpRvujCKlcgF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agamhub/Spark-SQL/blob/main/SparkStandalone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Spark"
      ],
      "metadata": {
        "id": "5RAsMEEwr4-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Google Colab Documentation GCS](https://colab.research.google.com/notebooks/io.ipynb)\n",
        "\n"
      ],
      "metadata": {
        "id": "FFAndBGg_ko9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **List Of To-Do**\n",
        "\n",
        "‚úÖ Spark DQC movement create multiple EXTERNAL table in 1 sessions of spark\n",
        "\n",
        "‚úÖ Connection to GCS\n",
        "\n",
        "‚úÖ Switch Postgres Airflow DB under docker to Local DB configuration\n",
        "\n",
        "‚úÖ PowerBI Airflow monitoring log\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "00tXv9FzAsjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Spark\n",
        "Setup Apache Spark in 1Ô∏è‚É£ 2Ô∏è‚É£ 3Ô∏è‚É£ 4Ô∏è‚É£ steps (step 0Ô∏è‚É£ is the Java installation, which is skipped because Java is available in Google Colab).\n",
        "\n",
        "The following code should also run on any Ubuntu machine or Docker container except for the Web servers links."
      ],
      "metadata": {
        "id": "bMEW3aR1r-Vy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "DchIK_2arhCO",
        "outputId": "2246a2eb-33e7-4254-ae1b-214ea402d1f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0Ô∏è‚É£   Install Java if not available\n",
            "‚úÖ Java is already installed.\n",
            "\n",
            "1Ô∏è‚É£   Download and install Hadoop and Spark\n",
            "‚úÖ https://dlcdn.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz was found\n",
            "‚úÖ File ‚Äòspark-3.5.4-bin-hadoop3.tgz‚Äô already there; not retrieving.\n",
            "‚úÖ Folder already exists\n",
            "\n",
            "2Ô∏è‚É£   Start Spark engine\n",
            "‚úÖ stopping org.apache.spark.deploy.master.Master\n",
            "‚úÖ starting org.apache.spark.deploy.master.Master, logging to /content/spark-3.5.4-bin-hadoop3/logs/spark--org.apache.spark.deploy.master.Master-1-e692a16b1cfc.out\n",
            "‚úÖ stopping org.apache.spark.deploy.worker.Worker\n",
            "‚úÖ starting org.apache.spark.deploy.worker.Worker, logging to /content/spark-3.5.4-bin-hadoop3/logs/spark--org.apache.spark.deploy.worker.Worker-1-e692a16b1cfc.out\n",
            "\n",
            "3Ô∏è‚É£   Start Master Web UI\n",
            "Search for port number in log file /content/spark-3.5.4-bin-hadoop3/logs/spark--org.apache.spark.deploy.master.Master-1-e692a16b1cfc.out\n",
            "‚úÖ Master UI is available at localhost:8081 (attempt nr. 2)\n",
            "Click on the link below to open the Spark Web UI üöÄ\n",
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(8081, \"/\", \"https://localhost:8081/\", window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4Ô∏è‚É£   Start history server\n",
            "‚úÖ stopping org.apache.spark.deploy.history.HistoryServer\n",
            "‚úÖ starting org.apache.spark.deploy.history.HistoryServer, logging to /content/spark-3.5.4-bin-hadoop3/logs/spark--org.apache.spark.deploy.history.HistoryServer-1-e692a16b1cfc.out\n",
            "Click on the link below to open the Spark History Server Web UI üöÄ\n",
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(18080, \"/\", \"https://localhost:18080/\", window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import requests\n",
        "import subprocess\n",
        "import os\n",
        "import re\n",
        "import socket\n",
        "import shutil\n",
        "import time\n",
        "import sys\n",
        "\n",
        "def run(cmd):\n",
        "    # run a shell command\n",
        "    try:\n",
        "        # Run the command and capture stdout and stderr\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        # Access stdout (stderr redirected to stdout)\n",
        "        stdout_result = subprocess_output.stdout.strip().splitlines()[-1]\n",
        "        # Process the results as needed\n",
        "        print(f'‚úÖ {stdout_result}')\n",
        "        return stdout_result\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(f\"Command failed with return code {e.returncode}\")\n",
        "        print(\"stdout:\", e.stdout)\n",
        "\n",
        "def is_java_installed():\n",
        "    return shutil.which(\"java\")\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "    os.environ['JAVA_HOME'] = ' /usr/lib/jvm/java-19-openjdk-amd64'\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(f'‚úÖ Done installing Java {java_version}')\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(f\"Command failed with return code {e.returncode}\")\n",
        "        print(\"stdout:\", e.stdout)\n",
        "\n",
        "print(\"\\n0Ô∏è‚É£   Install Java if not available\")\n",
        "if is_java_installed():\n",
        "    print(\"‚úÖ Java is already installed.\")\n",
        "else:\n",
        "    install_java()\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£   Download and install Hadoop and Spark\")\n",
        "# URL for downloading Hadoop and Spark\n",
        "SPARK_VERSION = \"3.5.4\"\n",
        "HADOOP_SPARK_URL = \"https://dlcdn.apache.org/spark/spark-\" + SPARK_VERSION + \\\n",
        "                   \"/spark-\" + SPARK_VERSION + \"-bin-hadoop3.tgz\"\n",
        "r = requests.head(HADOOP_SPARK_URL)\n",
        "if r.status_code >= 200 and r.status_code < 400:\n",
        "    print(f'‚úÖ {HADOOP_SPARK_URL} was found')\n",
        "else:\n",
        "    SPARK_CDN = \"https://dlcdn.apache.org/spark/\"\n",
        "    print(f'‚ö†Ô∏è {HADOOP_SPARK_URL} was NOT found. \\nCheck for available Spark versions in {SPARK_CDN}')\n",
        "\n",
        "# set some environment variables\n",
        "os.environ['SPARK_HOME'] = os.path.join(os.getcwd(), os.path.splitext(os.path.basename(HADOOP_SPARK_URL))[0])\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['SPARK_HOME'], 'bin'), os.environ['PATH']])\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['SPARK_HOME'], 'sbin'), os.environ['PATH']])\n",
        "\n",
        "# download Spark\n",
        "# using --no-clobber option will prevent wget from downloading file if already present\n",
        "# shell command: wget --no-clobber $HADOOP_SPARK_URL\n",
        "cmd = f\"wget --no-clobber {HADOOP_SPARK_URL}\"\n",
        "run(cmd)\n",
        "\n",
        "# uncompress\n",
        "try:\n",
        "    # Run the command and capture stdout and stderr\n",
        "    cmd = \"([ -d $(basename {0}|sed 's/\\.[^.]*$//') ] && echo -n 'Folder already exists') || (tar xzf $(basename {0}) && echo 'Uncompressed Spark distribution')\"\n",
        "    subprocess_output = subprocess.run(cmd.format(HADOOP_SPARK_URL), shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    # Access stdout (stderr redirected to stdout)\n",
        "    stdout_result = subprocess_output.stdout\n",
        "    # Process the results as needed\n",
        "    print(f'‚úÖ {stdout_result}')\n",
        "\n",
        "except subprocess.CalledProcessError as e:\n",
        "    # Handle the error if the command returns a non-zero exit code\n",
        "    print(f\"Command failed with return code {e.returncode}\")\n",
        "    print(\"stdout:\", e.stdout)\n",
        "\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£   Start Spark engine\")\n",
        "# start master\n",
        "# shell command: $SPARK_HOME/sbin/start-master.sh\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'stop-master.sh')\n",
        "run(cmd)\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'start-master.sh')\n",
        "out = run(cmd)\n",
        "\n",
        "# start one worker (first stop it in case it's already running)\n",
        "# shell command: $SPARK_HOME/sbin/start-worker.sh spark://${HOSTNAME}:7077\n",
        "cmd = [os.path.join(os.environ['SPARK_HOME'], 'sbin', 'stop-worker.sh')]\n",
        "run(cmd)\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'start-worker.sh') + ' ' + 'spark://'+socket.gethostname()+':7077'\n",
        "run(cmd)\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£   Start Master Web UI\")\n",
        "# get master UI's port number\n",
        "# the subprocess that's starting the master with start-master.sh\n",
        "# might still not be ready with assigning the port number at this point\n",
        "# therefore we check the logfile a few times (attempts=5) to see if the port\n",
        "# has been assigned. This might take 1-2 seconds.\n",
        "\n",
        "master_log = out.partition(\"logging to\")[2].strip()\n",
        "print(\"Search for port number in log file {}\".format(master_log))\n",
        "attempts = 10\n",
        "search_pattern = \"Successfully started service 'MasterUI' on port (\\d+)\"\n",
        "found = False\n",
        "for i in range(attempts):\n",
        "  if not found:\n",
        "   with open(master_log) as log:\n",
        "      found = re.search(search_pattern, log.read())\n",
        "      if found:\n",
        "          webUIport = found.group(1)\n",
        "          print(f\"‚úÖ Master UI is available at localhost:{webUIport} (attempt nr. {i})\")\n",
        "          break\n",
        "      else:\n",
        "          time.sleep(2) # need to try until port information is found in the logfile\n",
        "          i+=1\n",
        "if not found:\n",
        "  print(\"Could not find port for Master Web UI\\n\")\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    # serve the Web UI on Colab\n",
        "    print(\"Click on the link below to open the Spark Web UI üöÄ\")\n",
        "    from google.colab import output\n",
        "    output.serve_kernel_port_as_window(webUIport)\n",
        "\n",
        "print(\"\\n4Ô∏è‚É£   Start history server\")\n",
        "# start history server\n",
        "# shell command: mkdir -p /tmp/spark-events\n",
        "# shell command: $SPARK_HOME/sbin/start-history-server.sh\n",
        "spark_events_dir = os.path.join('/tmp', 'spark-events')\n",
        "if not os.path.exists(spark_events_dir):\n",
        "    os.mkdir(spark_events_dir)\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'stop-history-server.sh')\n",
        "run(cmd)\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'start-history-server.sh')\n",
        "run(cmd)\n",
        "\n",
        "if IN_COLAB:\n",
        "    # serve the History Server\n",
        "    print(\"Click on the link below to open the Spark History Server Web UI üöÄ\")\n",
        "    output.serve_kernel_port_as_window(18080)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start and Stop Worker\n",
        "\n"
      ],
      "metadata": {
        "id": "C5YU-yfN95MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cmd = [os.path.join(os.environ['SPARK_HOME'], 'sbin', 'stop-worker.sh')]\n",
        "run(cmd)\n",
        "cmd = os.path.join(os.environ['SPARK_HOME'], 'sbin', 'start-worker.sh') + ' ' + 'spark://'+socket.gethostname()+':7077'\n",
        "run(cmd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ypm_6A6z9BF4",
        "outputId": "a0276462-cf6e-4a2f-ca8b-c630d79bf08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ stopping org.apache.spark.deploy.worker.Worker\n",
            "‚úÖ starting org.apache.spark.deploy.worker.Worker, logging to /content/spark-3.5.4-bin-hadoop3/logs/spark--org.apache.spark.deploy.worker.Worker-1-e692a16b1cfc.out\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'starting org.apache.spark.deploy.worker.Worker, logging to /content/spark-3.5.4-bin-hadoop3/logs/spark--org.apache.spark.deploy.worker.Worker-1-e692a16b1cfc.out'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark with python & java examples"
      ],
      "metadata": {
        "id": "cRnaJXV314Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "EXAMPLES_JAR=$(find $SPARK_HOME/examples/jars/ -name \"spark-examples*\")\n",
        "\n",
        "$SPARK_HOME/bin/spark-submit \\\n",
        "  --class org.apache.spark.examples.SparkPi \\\n",
        "  --master spark://${HOSTNAME}:7077 \\\n",
        "  --conf spark.eventLog.enabled=true \\\n",
        "  $EXAMPLES_JAR \\\n",
        "  100 \\\n",
        "  2>/tmp/SparkPi_bash.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxzeGAWhu4CD",
        "outputId": "e1c34e71-6980-4460-8867-8b636cb513a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pi is roughly 3.1413179141317915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "EXAMPLES_PY=$(find $SPARK_HOME/examples/src/main/python/ -name \"wordcount.py\")\n",
        "\n",
        "echo $EXAMPLES_PY\n",
        "\n",
        "file=(\"/content/wordcount.txt\")\n",
        "\n",
        "echo $file\n",
        "\n",
        "$SPARK_HOME/bin/spark-submit \\\n",
        "  --master spark://${HOSTNAME}:7077 \\\n",
        "  --name \"GOOGLECOLAB\" \\\n",
        "  --driver-memory 1g \\\n",
        "  --num-executors 1 \\\n",
        "  --executor-memory 1024M \\\n",
        "  --conf spark.cores.max=2 \\\n",
        "  $EXAMPLES_PY $file 2> /tmp/pyPI.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghpNrPqlvrQQ",
        "outputId": "a1936fb1-5ed0-446a-99ed-980d7f2a2fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark-3.5.4-bin-hadoop3/examples/src/main/python/wordcount.py\n",
            "/content/wordcount.txt\n",
            "halo: 1\n",
            "nama: 1\n",
            "saya: 1\n",
            "agam: 1\n",
            "adhinegara: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NGROK = True #@param {type:\"boolean\"}"
      ],
      "metadata": {
        "id": "gnQpXQRB5l4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if NGROK:\n",
        "  !pip install pyngrok\n",
        "  from pyngrok import ngrok, conf\n",
        "  import getpass\n",
        "\n",
        "  print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "  authtoken = getpass.getpass()\n",
        "  conf.get_default().auth_token = authtoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYuAPr8j520r",
        "outputId": "3c016d05-19aa-47a2-91bc-799e8d554c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/get-started/your-authtoken\n",
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if NGROK:\n",
        "  # close all existing connections (https://pyngrok.readthedocs.io/en/latest/#get-active-tunnels)\n",
        "  tunnels = ngrok.get_tunnels()\n",
        "  if tunnels:\n",
        "    map(lambda t: ngrok.disconnect(t.public_url), tunnels)\n",
        "  # Open a ngrok tunnel to the HTTP server\n",
        "  public_url = ngrok.connect(8081).public_url\n",
        "  print(f'Click on {public_url} to open the Spark Master Web UI')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEI47xJK60MK",
        "outputId": "59a77220-488e-4f54-ebfb-e6baf4c224c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click on https://a839-34-135-15-192.ngrok-free.app to open the Spark Master Web UI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if NGROK:\n",
        "  # Open a ngrok tunnel to the HTTP server\n",
        "  public_url = ngrok.connect(18080).public_url\n",
        "  print(f'Click on {public_url} to open the Spark Master Web UI')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlrTDN5a6jo7",
        "outputId": "c760b681-532d-44e5-fbf5-abbc0c86fe09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click on https://765f-34-135-15-192.ngrok-free.app to open the Spark Master Web UI\n"
          ]
        }
      ]
    }
  ]
}